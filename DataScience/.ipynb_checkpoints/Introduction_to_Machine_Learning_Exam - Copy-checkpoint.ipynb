{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUbQ9ggKpRN_"
   },
   "source": [
    "This Exam has **TWO** sections: \n",
    "\n",
    "Section **A** Contains questions that require short but, real world case scanarios.\n",
    "\n",
    "Section **B** is a coding section. The last section (**B**) seeks to exam application of theoretical problem statements in Machine Learning.\n",
    "\n",
    "Your are required to Answer : \n",
    "\n",
    "**All QUESTIONS IN SECTION A & AT LEAST TWO CODING QUESTIONS IN SECTION B**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yTsx-9sutUJ"
   },
   "source": [
    "***SECTION A*** (Attempt all Questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jszvYezlpUuF"
   },
   "source": [
    "**Question one.** \n",
    "\n",
    "A Classification algorithm tries to determine the class or category of the data is presented with. Many times, an object might belong to several categories, and the AI needs to determine what those categories are and how much confidence the algorithm has its predictions.\n",
    "\n",
    "**(a)** Explain one real world case scenario where a classification algorithm can be applied to solve a problem. **(10 marks)**\n",
    "\n",
    "**(b)** Describe at least two types of variables used in classification algorithms from the scanario explained in (a) above **(5 marks)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWiJu5Ris42C"
   },
   "source": [
    "**Question Two**\n",
    "\n",
    "The fundemental issue in Machine Learning is the tension between optimization and generalization. **Optimization** refers to the process of adjusting the model to get the best performance possible on the training dataset (learning in machine learning) WHILE **Generalization** refers to how well the trained model performs on data it has never seen before.The goal of the game is to get good generalization, of course, but you donâ€™t control generalization;you can only adjust the model based on its training data.\n",
    "\n",
    " Drawing from a real world examples on models: - predicting movie reviews, topic classification, and house-price regression. \n",
    "\n",
    "**(a)** Explain how the performance of the models can be diagnosed to\n",
    "improve their power. (10 marks)\n",
    "\n",
    "**(b)** Explain the meaning of the following terms as used in Model Evaluation Metrics: \n",
    "\n",
    "(i) The mean absolute error (2 marks)\n",
    "\n",
    "(ii) Explained variance score: (Hint: 1 means perfect prediction & 0 otherwise) (3 marks) \n",
    "\n",
    "(iii) The mean squared error (MSE) (3 marks) \n",
    "\n",
    "(iv) Root-Mean-Squared-Error (RMSE) (2 marks)\n",
    "\n",
    "(v) Receiver Operating Characteristic curve (ROC) (5 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oclZIroSuzXw"
   },
   "source": [
    "***SECTION B*** (Choose at least two questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXsLv4StyEVQ"
   },
   "source": [
    "## **Question One **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R42loV0Xu33O"
   },
   "outputs": [],
   "source": [
    "# start by importing all key essential libraries and dependencies to use in your notebook at once. \n",
    "\n",
    "# This action will be awarded marks depending on the use of library (ies) imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VVmlLBKzvyGd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from ml_metrics import rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Egk76Nkv3iq"
   },
   "outputs": [],
   "source": [
    "# import the datasets (train and test datasets in the environment)\n",
    "\n",
    "train = pd.read_csv(\"../input/neolen-house-price-prediction/train.csv\") # Note that this code doesnot work on your pc, but, points you to something \n",
    "\n",
    "test = pd.read_csv(\"../input/neolen-house-price-prediction/test.csv\") # Note that this code doesnot work on your pc, but, points you to something "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1R0VqTzwgQT"
   },
   "outputs": [],
   "source": [
    "# Write a simple piece of code to Sneak-Peek into Datasets\n",
    "\n",
    "# Wrute code to show Columns Names\n",
    "\n",
    "# Column Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSbkmGfAwnSx"
   },
   "outputs": [],
   "source": [
    "# *Extracting Primary Information*\n",
    "\n",
    "# Write A Simple line (s) of code that extracts key primary information from the dataset(s):\n",
    "\n",
    "# 1. The Total number of datapoints are?\n",
    "\n",
    "# 2. The Total number of columns are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IxYG44E6w0t2"
   },
   "outputs": [],
   "source": [
    "# Describe the datasets in your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVGYPSl8xAc-"
   },
   "outputs": [],
   "source": [
    "# Write pieces of code to check the following in the datasets:\n",
    "\n",
    "# 1. na-values checking\n",
    "\n",
    "# 2. null-values check\n",
    "\n",
    "# 3. Redundant Data Removal\n",
    "\n",
    "# 4. perform other preliminary data cleaning and pre-processing as you may deam neccessary before fitting any models (10 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJmvio5HxYsp"
   },
   "outputs": [],
   "source": [
    "# Using any two features of your choice. \n",
    "\n",
    "# Fit a linear regression model to minimize the cost function under linear regression algorithm \n",
    "\n",
    "# (Hint: The cost function under linear regression algorithm is to minimize the squared sums of squares)\n",
    "\n",
    "# What do your results mean? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0xwvG6Lx8lM"
   },
   "source": [
    "## **Question two **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W96jUh6aT2vk"
   },
   "source": [
    "# Folowing the conventional four steps for fitting a model in machine learning\n",
    "# 1. Choose the model class\n",
    "# 2. Instantiate the model with hyperparameter\n",
    "# 3. Fit to data. Observe y \n",
    "# 4. Do model diagnostics or model evaluation\n",
    "# Use the iris dataset to fit a classification algorithm \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXDeD2TWUnsd"
   },
   "source": [
    "## **Question three **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYgETKq6V2DZ"
   },
   "source": [
    "**Model validation the right way**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21_RW26wV5gT"
   },
   "outputs": [],
   "source": [
    "# Load the iris dataset \n",
    "\n",
    "# follow the sample code below to split the dataset into 50% in each set \n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "# split the data with 50% in each set\n",
    "X1, X2, y1, y2 = train_test_split(X, y, random_state=0,\n",
    "                                  train_size=0.5)\n",
    "\n",
    "# fit the model on one set of data\n",
    "model.fit(X1, y1)\n",
    "\n",
    "# evaluate the model on the second set of data\n",
    "y2_model = model.predict(X2)\n",
    "accuracy_score(y2, y2_model)\n",
    "\n",
    "# fit the models \n",
    "\n",
    "y2_model = model.fit(X1, y1).predict(X2)\n",
    "y1_model = model.fit(X2, y2).predict(X1)\n",
    "accuracy_score(y1, y1_model), accuracy_score(y2, y2_model)\n",
    "\n",
    "# do cross validation \n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "# REQUIRED FROM YOU!\n",
    "\n",
    "# Write a piece of code to accomplish :\n",
    "\n",
    "# 1. Selecting the best model from the above data split  by creating the bias variance trade off curves\n",
    "\n",
    "# 2. Create a learning curve graph in scikit -learn \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
